---
description: Guides Perplexity analysis for improving codebase stability
globs: **/*.py", "**/requirements.txt", "**/config/*.yaml", "**/*.toml
---
name: "GitHub Model Conversion Protocol"
version: "1.2"
strict_mode: true

core_principles:
  testing:
    mock_testing: false
    requirements:
      - use_live_apis: true
      - use_production_credentials: true
      - real_time_data: true
      - direct_api_calls: true
    
  cli_integration:
    mirror_ui: true
    rules:
      - use_identical_code_paths: true
      - use_same_api_endpoints: true
      - use_same_model_config: true
      - no_test_mode: true
      - no_mock_data: true

conversion_rules:
  ai_integration:
    - pattern: "from openai import"
      replacement: "from transformers import AutoModelForCausalLM, AutoTokenizer"
      validation:
        - check: "AutoModelForCausalLM" in code
        - check: "AutoTokenizer" in code
        - required_files: ["./model_configs/production.yaml"]

error_handling:
  validations:
    - name: "response_schema_check"
      pattern: "def process_ai_response"
      required: |
        from pydantic import BaseModel
        
        class AIResponseSchema(BaseModel):
            content: str
            confidence: float
            error: Optional[str]

stability_rules:
  memory_management:
    - context_managers_required: true
    - cleanup_required: true
    
  error_recovery:
    - pattern: "except Exception as e"
      required: |
        except (ModelRuntimeError, ConnectionError) as e:
            logging.error(f"[CRITICAL] {str(e)}")
            raise ServiceDegradationError from e

monitoring:
  metrics:
    - error_rate
    - latency_p95
    - memory_usage
    - token_usage
  alerts:
    error_rate_threshold: 0.05
    latency_threshold: 2000
    memory_threshold: 0.9

documentation:
  required_sections:
    - "API Integration"
    - "Model Configuration"
    - "Error Handling"
    - "Performance Monitoring"